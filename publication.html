<tr>
    <td class="header">
        Publications
        <button class="pub-toggle-btn-global"></button>
    </td>
</tr>
<tr>
    <td>
        <table class="table-contents">
            <tbody>
                <tr>
                    <tr class="tr-pub-main">
                        <td class="tb-td-left">
                            <img class="img-zoom" src="src/papers/imgs/3dsuperres.jpg">
                        </td>
                        <td class="tb-td-right">
                            <span class="paper-title">Near-realtime Facial Animation by Deep 3D Simulation Super-resolution</span>
                            <br>
                            <span class="my-author">Hyojoon Park</span>, Sangeetha Grama Srinivasan, Matthew Cong, Doyub Kim, Byungsoo Kim, Jonathan Swartz, Ken Museth, and Eftychios Sifakis
                            <br>
                            <span class="conference">ACM Transactions on Graphics (TOG) 2024</span>
                            <br>
                            <a href="https://dl.acm.org/doi/full/10.1145/3670687" target="">paper</a>
                            |
                            <a href="https://github.com/hjoonpark/3d-sim-super-res" target="">github</a>
                            |
                            presented at <a href="https://asia.siggraph.org/2024/session/?sess=sess117" target="">SIGGRAPH ASIA 2024</a>
                            <br><button class="pub-toggle-btn"></button>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2">
                            <div class="pub-toggle-content">
                                <ul>
                                    <li>3D simulation super-resolution achieving high-resolution facial animation 115x faster (at 18 FPS) than traditional method while maintaining similar quality.</li>
                                    <li>Novel deep learning architecture (including point-cloud upsampling with arbitrary and non-integer upsampling ratios) to enhance low-resolution simulations and generalize to unseen expressions.</li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                </tr>
                <tr>
                    <tr class="tr-pub-main">
                        <td class="tb-td-left">
                            <img class="img-zoom" src="src/papers/imgs/media2022.jpg">
                        </td>
                    <td class="tb-td-right">
                            <span class="paper-title">Collagen Fiber Centerline Tracking in Fibrotic Tissue via Deep Neural Networks with Variational Autoencoder-based Synthetic Training Data Generation</span>
                            <br>
                            <span class="my-author">Hyojoon Park</span>*, Bin Li*, Yuming Liu, Michael S. Nelson, Helen M. Wilson, Eftychios Sifakis, and Kevin W. Eliceiri. (* equal contributions)
                            <br>
                            <span class="conference">Medical Image Analysis 2023</span> 
                            <br>
                            <a href="https://www.sciencedirect.com/science/article/pii/S1361841523002219" target="">paper</a>
                            |
                            <a href="https://github.com/uw-loci/collagen-fiber-metrics.git" target="">github</a>
                            <br><button class="pub-toggle-btn"></button>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2">
                            <div class="pub-toggle-content">
                                <ul>
                                    <li>Introduces a novel property-controllable variational autoencoder, DuoVAE, designed to generate synthetic data with desired properties.</li>
                                    <li>Facilitates the creation of diverse synthetic datasets, addressing the challenge of limited annotated data for training deep learning models.</li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                </tr>
                <tr>
                    <tr class="tr-pub-main">
                        <td class="tb-td-left">
                            <img class="img-zoom" src="src/papers/imgs/tog21.JPG">
                        </td>
                    <td class="tb-td-right">
                            <span class="paper-title">Capturing Detailed Deformations of Moving Human Bodies</span>
                            <br>
                            He Chen, <span class="my-author">Hyojoon Park</span>, Kutay Macit, and Ladislav Kavan
                            <br>
                            <span class="conference">ACM SIGGRAPH 2021</span>
                            <br>
                            <a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Capturing+Detailed+Deformations+of+Moving+Human+Bodies&btnG=" target="">paper</a>
                            |
                            <a href="https://ankachan.github.io/Projects/MocapCheckerboard/MocapCheckerboard.html" target="">project page</a>
                            |
                            <a href="https://github.com/hjoonpark/MultiCamCalib" target="">multi-camera calibration codes</a>
                            <br><button class="pub-toggle-btn"></button>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2">
                            <div class="pub-toggle-content">
                                <ul>
                                    <li>Introduces a novel motion capture system in a multi-camera setup using a specially designed low-cost suit, enabling the accurate 3D reconstruction of a moving human body.</li>
                                    <li>Uses deep neural networks and geometric algorithms for accurate corner detection, labeling, and 3D reconstruction.</li>
                                    <li>I developed and deployed the multi-camera calibration framework enhanced by machine learning-based anomaly corner detection.</li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                </tr>
                <tr>
                    <tr class="tr-pub-main">
                        <td class="tb-td-left">
                            <img class="img-zoom" src="src/papers/imgs/iros2020.jpg">
                        </td>
                    <td class="tb-td-right">
                            <span class="paper-title">Adaptive Precision-Enhancing Hand Rendering for Wearable Fingertip Tracking Devices</span>
                            <br>
                            <span class="my-author">Hyojoon Park</span> and Jung-Min Park
                            <br>
                            <span class="conference">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020</span>
                            <br>
                            <a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C45&q=Adaptive+Precision-Enhancing+Hand+Rendering+for+Wearable+Fingertip+Tracking+Devices&btnG=" target="">paper</a>
                            |
                            <a href="https://youtu.be/_JeicPxjsXQ" target="">short video</a>
                            <br><button class="pub-toggle-btn"></button>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2">
                            <div class="pub-toggle-content">
                                <ul>
                                    <li>Introduces 3D hand rendering framework in VR for wearable fingertip tracking devices, focusing on reconstructing realistic hand poses using only fingertip positions. </li>
                                    <li>Key contributions include motion retargeting and new hinge constraints for real-time inverse kinematics to enhance visual plausibility and precision.</li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                </tr>
                <tr>
                    <tr class="tr-pub-main">
                        <td class="tb-td-left">
                            <img class="img-zoom" src="src/papers/imgs/dental.PNG">
                        </td>
                        <td class="tb-td-right">
                            <span class="paper-title">Dental Simulator with Increased Z-width of Haptic Rendering</span>
                        <br>
                        <span class="my-author">Hyojoon Park</span>, Myungsin Kim, and Dongjun Lee
                        <br>
                        <span class="conference">AsiaHaptics 2018</span>
                        <br>
                        <a  href="src/papers/ThesisContents.pdf" target="">MS thesis</a>
                        |
                        <a  href="src/papers/HPark_AsiaHaptics18_ShortPaper.pdf" target="">short paper</a>
                        |
                        <a href="https://youtu.be/R7unVX3cIXg" target="">short video</a>
                        |
                        <a href="src/papers/videos/master_degree_compressed.mp4" target="">long video</a>[*]
                        <br><br>
                        [*] <a href="src/papers/snu_outstanding_ms_thesis.jpeg">Outstanding MS Thesis Presentation Award</a> (Seoul National University 2019)
                            <br><button class="pub-toggle-btn"></button>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2">
                            <div class="pub-toggle-content">
                                <ul>
                                    <li>Introduces a VR dental simulator that renders highly stiff haptic feedback, achieving up to 10× greater maximum stiffness (Z-width) from virtual teeth using commercially available haptic devices.</li>
                                    <!-- <li>Employs a passive midpoint integrator (PMI) and virtual coupling with passive decomposition to significantly increase the maximum achievable stiffness (Z-width).</li> -->
                                </ul>
                            </div>
                        </td>
                    </tr>
                </tr>
                <tr>
                    <tr class="tr-pub-main">
                        <td class="tb-td-left">
                            <img class="img-zoom" src="src/papers/imgs/glove.PNG">
                        </td>
                    <td class="tb-td-right">
                            <span class="paper-title">Stretchable Skin-Like Cooling/Heating Device for Reconstruction of Artificial Thermal Sensation in Virtual Reality</span>
                            <br>
                            Jnwoo Lee, Heayoun Sul, Wonha Lee, Kyung Rok Pyun, Inho Ha, Dongkwan Kim, <span class="my-author">Hyojoon Park</span>, Hyeonjin Eom, Yeosang Yoon, Jinwook Jung, Dongjun Lee, and Seung Hwan Ko
                            <br>
                            <span class="conference">Advanced Functional Materials 2020</span>
                            <br>
                            <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/adfm.201909171" target="">paper</a>
                            <br><button class="pub-toggle-btn"></button>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2">
                            <div class="pub-toggle-content">
                                <ul>
                                    <li>Introduces a stretchable, bi-functional skin-like thermo-haptic (STH) device for VR, capable of precise cold and hot sensations with a single structure and 230% stretchability.</li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                </tr>
                <tr>
                <tr class="tr-pub-main">
                    <td class="tb-td-left">
                        <img class="img-zoom" src="src/papers/imgs/collaborative.PNG">
                    </td>
                    <td class="tb-td-right">
                        <span class="paper-title">Rigid-body Collaborative Manipulation among Remote Users with Wearable Cutaneous Haptic Interfaces</span>
                        <br>
                        Myungsin Kim, WonHa Lee, <span class="my-author">Hyojoon Park</span>, Junghan Kwon, Yong-Lae Park, and Dongjun Lee
                        <br>
                        <span class="conference">AsiaHaptics 2018</span>
                        <br>
                        <a  href="src/papers/MKim_AsiaHaptics18.pdf" target="">paper</a>
                        |
                        <a href="src/papers/videos/low_AsiaHaptics2018_KMS.mp4" target="">video</a>
                        <br><button class="pub-toggle-btn"></button>
                    </td>
                </tr>
                <tr>
                    <td colspan="2">
                        <div class="pub-toggle-content">
                            <ul>
                                <li>Introduces a remote multiuser collaboration system via wearable cutaneous haptic interfaces, leveraging passivity-based simulations for stable and realistic interactions.</li>
                            </ul>
                        </div>
                    </td>
                </tr>
            </tr>
            <tr>
                <tr class="tr-pub-main">
                    <td class="tb-td-left">
                        <img class="img-none" src="src/empty.png">
                    </td>
                    <td class="tb-td-right">
                        <span class="paper-title">Design and Performance Evaluation of Wearable Haptic Interfaces</span>
                        <br>
                        WonHa Lee, Myungsin Kim, <span class="my-author">Hyojoon Park</span>, and Dongjun Lee
                        <br>
                        <span class="conference">International Conference on Control, Automation and Systems 2018</span>
                        <br>
                    </td>
                </tr>
            </tr>
            <tr>
                <tr class="tr-pub-main">
                    <td class="tb-td-left">
                        <img class="img-none" src="src/empty.png">
                    </td>
                    <td class="tb-td-right">
                        <span class="paper-title">Wearable Cutaneous Haptic Interface with Soft Sensors and IMUs</span>
                        <br>
                        Yongjun Lee, Myungsin Kim, Yongseok Lee, <span class="my-author">Hyojoon Park</span>, and Dongjun Lee
                        <br>
                        <span class="conference">Korea Robotics Society Annual Conference 2018</span>
                        <br>
                    </td>
                </tr>
            </tr>
            </tbody>
        </table>
    </td>
</tr>